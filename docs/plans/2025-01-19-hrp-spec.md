# Hedgefund Research Platform (HRP) Specification

## Section 1: Vision & Principles

**Project Codename:** Hedgefund Research Platform (HRP)

**Mission:** A personal, professional-grade quantitative research platform that enables systematic discovery, validation, and deployment of long-only equity strategies with institutional rigor‚Äîbuilt lean by integrating best-in-class existing tools.

### Implementation Status (as of January 2026)

**Current State:** ~17,500 lines of production code across 80+ modules.

| Tier | Focus | Status | Completion |
|------|-------|--------|------------|
| **Foundation** | Data + Research Core | ‚úÖ Complete | 100% |
| **Intelligence** | ML + Agents | üü° Active | 85% |
| **Production** | Security + Ops | ‚è≥ Planned | 0% |
| **Trading** | Live Execution | üîÆ Future | 0% |

**Tier Mapping from Original Phases:**
- Phases 0-4 ‚Üí Tier 1: Foundation (Database, API, Dashboard, Data Pipeline)
- Phases 5-8 ‚Üí Tier 2: Intelligence (ML Framework, Agents, Validation)
- Future ‚Üí Tier 3: Production (Security, Monitoring)
- Future ‚Üí Tier 4: Trading (IBKR, Paper/Live)

**Test Suite:** 1,198 tests with 97.6% pass rate.

**Documentation:** See `docs/plans/Project-Status.md` for detailed tier-based progress tracking.

### Core Principles

1. **Research-First** ‚Äî Every trade idea starts as a hypothesis with a documented thesis, testable predictions, and falsification criteria. No "I think this works" without evidence.

2. **Reproducibility** ‚Äî Every experiment is versioned, logged, and re-runnable. Code, data snapshots, parameters, and results are immutably linked. You can recreate any past result.

3. **Statistical Rigor** ‚Äî Walk-forward validation, out-of-sample testing, and multiple hypothesis correction are enforced by default. The system assumes you'll overfit and protects you from yourself.

4. **Audit Trail** ‚Äî Every decision, model version, and signal has lineage. If a strategy is deployed, you can trace exactly why it qualified‚Äîand why alternative strategies were rejected. Comparative reasoning, not cherry-picking.

5. **Leverage Existing Tools** ‚Äî Don't rebuild what exists. Use VectorBT for backtesting, DuckDB for data, MLflow for experiments, Streamlit for dashboards. Build custom only where no good solution exists (agent integration, audit/lineage, orchestration).

6. **Hybrid Strategy Intelligence** ‚Äî Combine ML-driven discovery with systematic application of established strategies (momentum, mean reversion, factor models). Classic strategies serve as baselines; agents surface novel hypotheses.

7. **Agent-Native** ‚Äî AI agents are first-class citizens. Interactive research via MCP + scheduled background agents for monitoring and discovery. Both use the same underlying APIs as the dashboard.

8. **Local-First** ‚Äî Runs entirely on your Mac Mini (M4, 16GB). Data stays private. No cloud dependencies for core functionality.

9. **Risk & Capital Discipline** ‚Äî All strategies evaluated with realistic IBKR transaction costs. Signal-scaled position sizing with configurable limits. Performance assessed on post-cost, capacity-aware metrics.

### Scope Constraints

| Dimension | In Scope | Out of Scope (for now) |
|-----------|----------|------------------------|
| **Asset class** | US equities | ETFs (except benchmarks), crypto, futures, options |
| **Direction** | Long-only | Short selling, pairs trading |
| **Timeframe** | Daily (end-of-day signals) | Intraday, tick-level |
| **Universe** | S&P 500 initially, expand to Russell 1000/3000 | International equities |
| **Exclusions** | Financials, REITs, penny stocks (<$5) | ‚Äî |
| **Broker** | Interactive Brokers | Other brokers |
| **Deployment** | Paper trading initially | Live trading (future phase) |

---

## Section 2: Architecture Overview

### Design Philosophy

Three-layer architecture that separates concerns while remaining simple enough for one person to maintain. Each layer has clear responsibilities and communicates through well-defined interfaces.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CONTROL LAYER                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   Streamlit     ‚îÇ  ‚îÇ   MCP Servers   ‚îÇ  ‚îÇ   Scheduled     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   Dashboard     ‚îÇ  ‚îÇ   (Claude)      ‚îÇ  ‚îÇ   Agents        ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ           ‚îÇ                    ‚îÇ                    ‚îÇ          ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ                                ‚ñº                               ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îÇ
‚îÇ                    ‚îÇ   Platform API      ‚îÇ                     ‚îÇ
‚îÇ                    ‚îÇ   (Python module)   ‚îÇ                     ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    RESEARCH LAYER                               ‚îÇ
‚îÇ                                ‚ñº                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   VectorBT      ‚îÇ  ‚îÇ   MLflow        ‚îÇ  ‚îÇ   Hypothesis    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   (Backtest)    ‚îÇ  ‚îÇ   (Experiments) ‚îÇ  ‚îÇ   Registry      ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ           ‚îÇ                    ‚îÇ                    ‚îÇ          ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ                                ‚ñº                               ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îÇ
‚îÇ                    ‚îÇ   Audit/Lineage     ‚îÇ                     ‚îÇ
‚îÇ                    ‚îÇ   System            ‚îÇ                     ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    DATA LAYER                                   ‚îÇ
‚îÇ                                ‚ñº                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   DuckDB        ‚îÇ  ‚îÇ   Ingestion     ‚îÇ  ‚îÇ   Feature       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   (Storage)     ‚îÇ  ‚îÇ   (Pipelines)   ‚îÇ  ‚îÇ   Store         ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Layer Responsibilities

#### Data Layer
| Component | Responsibility | Implementation |
|-----------|----------------|----------------|
| **DuckDB** | Store price data, fundamentals, signals, results | Single `.duckdb` file, SQL interface |
| **Ingestion Pipelines** | Fetch, validate, and load data from sources | Python scripts, scheduled via cron |
| **Feature Store** | Pre-computed features (momentum, volatility, etc.) | DuckDB tables, versioned by date |

#### Research Layer
| Component | Responsibility | Implementation |
|-----------|----------------|----------------|
| **VectorBT** | Backtesting engine | Vectorized, fast, handles position sizing |
| **MLflow** | Experiment tracking, model registry | Local server, logs params/metrics/artifacts |
| **Hypothesis Registry** | Track research questions and their status | DuckDB table + markdown docs |
| **Audit/Lineage** | Link hypothesis ‚Üí experiment ‚Üí result ‚Üí decision | Custom, the core differentiator |

#### Control Layer
| Component | Responsibility | Implementation |
|-----------|----------------|----------------|
| **Platform API** | Single interface for all operations | Python module, used by all consumers |
| **Streamlit Dashboard** | Visualize data, experiments, results | Web UI on localhost |
| **MCP Servers** | Expose APIs to Claude for interactive research | FastMCP, tool definitions |
| **Scheduled Agents** | Background tasks: data refresh, monitoring, discovery | Python + Claude API, cron-triggered |

### Communication Patterns

1. **All consumers use the Platform API** ‚Äî Dashboard, MCP servers, and agents never touch DuckDB or VectorBT directly. This ensures consistency and auditability.

2. **Synchronous for interactive** ‚Äî Dashboard and MCP calls are synchronous, return results immediately.

3. **Async for batch** ‚Äî Data ingestion and scheduled agents run asynchronously, write results to DuckDB, send notifications on completion/failure.

4. **Everything is logged** ‚Äî Every API call is logged with timestamp, caller, parameters, and outcome.

### Key Design Decisions

| Decision | Rationale |
|----------|-----------|
| Single DuckDB file | Simple, portable, no database server to manage. DuckDB handles analytical queries well. |
| VectorBT over custom | Battle-tested, vectorized (fast), good pandas integration. Don't rebuild backtesting. |
| MLflow over custom | Industry standard, handles versioning and comparison. Overkill features can be ignored. |
| Platform API as Python module | No HTTP overhead for local use. MCP servers wrap it, dashboard imports it. |
| MCP for Claude integration | Native Claude Code support, tools are just Python functions. |

---

## Section 3: Data Layer

### Overview

The Data Layer handles all data acquisition, storage, and feature engineering. Built on DuckDB for simplicity and performance‚Äîno database server to manage, just a single file that handles analytical queries efficiently.

### Storage Schema

#### Core Tables

```sql
-- Universe: which symbols are tradeable and when
CREATE TABLE universe (
    symbol VARCHAR NOT NULL,
    date DATE NOT NULL,
    in_universe BOOLEAN DEFAULT TRUE,
    exclusion_reason VARCHAR,  -- 'financial', 'reit', 'penny_stock', 'delisted'
    PRIMARY KEY (symbol, date)
);

-- Daily OHLCV price data
CREATE TABLE prices (
    symbol VARCHAR NOT NULL,
    date DATE NOT NULL,
    open DECIMAL(12,4),
    high DECIMAL(12,4),
    low DECIMAL(12,4),
    close DECIMAL(12,4),
    adj_close DECIMAL(12,4),
    volume BIGINT,
    source VARCHAR,            -- 'polygon', 'yfinance', etc.
    ingested_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (symbol, date)
);

-- Fundamental data (quarterly, point-in-time)
CREATE TABLE fundamentals (
    symbol VARCHAR NOT NULL,
    report_date DATE NOT NULL,      -- when the data became known
    period_end DATE NOT NULL,       -- fiscal period end
    metric VARCHAR NOT NULL,        -- 'revenue', 'eps', 'pe_ratio', etc.
    value DECIMAL(18,4),
    source VARCHAR,
    ingested_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (symbol, report_date, metric)
);

-- Pre-computed features
CREATE TABLE features (
    symbol VARCHAR NOT NULL,
    date DATE NOT NULL,
    feature_name VARCHAR NOT NULL,  -- 'momentum_20d', 'volatility_60d', etc.
    value DECIMAL(18,6),
    version VARCHAR,                -- feature computation version
    computed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (symbol, date, feature_name, version)
);

-- Corporate actions for adjustment
CREATE TABLE corporate_actions (
    symbol VARCHAR NOT NULL,
    date DATE NOT NULL,
    action_type VARCHAR NOT NULL,   -- 'split', 'dividend', 'spinoff'
    factor DECIMAL(12,6),           -- split ratio or dividend amount
    source VARCHAR,
    PRIMARY KEY (symbol, date, action_type)
);
```

#### Metadata Tables

```sql
-- Data source registry
CREATE TABLE data_sources (
    source_id VARCHAR PRIMARY KEY,
    source_type VARCHAR,            -- 'price', 'fundamental', 'corporate_action'
    api_name VARCHAR,
    last_fetch TIMESTAMP,
    status VARCHAR                  -- 'active', 'failed', 'rate_limited'
);

-- Ingestion log
CREATE TABLE ingestion_log (
    log_id INTEGER PRIMARY KEY,
    source_id VARCHAR,
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    records_fetched INTEGER,
    records_inserted INTEGER,
    status VARCHAR,                 -- 'success', 'partial', 'failed'
    error_message VARCHAR
);
```

### Ingestion Pipelines

#### Pipeline Structure

```
data/
‚îú‚îÄ‚îÄ ingestion/
‚îÇ   ‚îú‚îÄ‚îÄ prices.py          # Fetch daily prices
‚îÇ   ‚îú‚îÄ‚îÄ fundamentals.py    # Fetch fundamental data
‚îÇ   ‚îú‚îÄ‚îÄ universe.py        # Update universe membership
‚îÇ   ‚îú‚îÄ‚îÄ corporate_actions.py
‚îÇ   ‚îî‚îÄ‚îÄ scheduler.py       # Cron-like scheduling
```

#### Ingestion Principles

1. **Idempotent** ‚Äî Running the same ingestion twice produces the same result. Use upserts, not inserts.

2. **Point-in-time aware** ‚Äî Fundamental data uses `report_date` (when known), not `period_end` (fiscal period). This prevents look-ahead bias.

3. **Source tracking** ‚Äî Every record knows where it came from and when it was ingested.

4. **Incremental** ‚Äî Only fetch new data, not full history on every run.

5. **Validation on ingest** ‚Äî Reject obviously bad data (negative prices, missing required fields, dates in future).

#### Daily Schedule

| Time | Job | Description |
|------|-----|-------------|
| 6:00 PM ET | `prices.py` | Fetch today's closing prices |
| 6:05 PM ET | `universe.py` | Update universe membership (S&P 500 changes) |
| 6:10 PM ET | `features.py` | Recompute features for today |
| Weekly (Sat) | `fundamentals.py` | Fetch latest fundamental data |

**Implementation:** Fully automated via `hrp/agents/scheduler.py` with `UniverseUpdateJob`, `PriceIngestionJob`, and `FeatureComputationJob`.

### Feature Store

Pre-computed features avoid redundant calculation during backtests and research.

#### Standard Features

| Feature | Computation | Lookback |
|---------|-------------|----------|
| `return_Nd` | (close[t] - close[t-N]) / close[t-N] | 1, 5, 20, 60, 252 days |
| `momentum_Nd` | return_Nd | 20, 60, 252 days |
| `volatility_Nd` | std(returns) * sqrt(252) | 20, 60 days |
| `volume_avg_Nd` | mean(volume) | 20 days |
| `volume_ratio` | volume / volume_avg_20d | ‚Äî |
| `rsi_Nd` | Relative Strength Index | 14 days |
| `sma_Nd` | Simple Moving Average | 20, 50, 200 days |
| `price_to_sma_Nd` | close / sma_Nd | 20, 50, 200 days |

#### Feature Versioning

Features are versioned to ensure reproducibility:
- `version` column tracks computation logic version
- Old versions retained for historical experiment reproduction
- New version computed alongside old until validated

### Data Quality

#### Automated Checks

| Check | Action |
|-------|--------|
| Missing prices for active symbol | Alert, mark data incomplete |
| Price change > 50% without corporate action | Flag for review |
| Negative prices or volume | Reject record |
| Duplicate records | Keep most recent ingestion |
| Gaps in date sequence | Alert, interpolation not automatic |

#### Quality Dashboard

Streamlit page showing:
- Ingestion status by source
- Data completeness by symbol
- Flagged anomalies requiring review
- Historical data quality trends

---

## Section 4: Research Engine

### Overview

The Research Engine is where hypotheses become evidence. Built on VectorBT for backtesting and MLflow for experiment tracking, with a custom Hypothesis Registry and Audit/Lineage system that ties everything together.

### Hypothesis Registry

Every research idea starts as a formal hypothesis before any code is written.

#### Hypothesis Schema

```sql
CREATE TABLE hypotheses (
    hypothesis_id VARCHAR PRIMARY KEY,  -- 'HYP-2025-001'
    title VARCHAR NOT NULL,
    thesis TEXT NOT NULL,               -- What you believe and why
    testable_prediction TEXT NOT NULL,  -- Specific, measurable outcome
    falsification_criteria TEXT,        -- What would prove this wrong
    status VARCHAR DEFAULT 'draft',     -- draft, testing, validated, rejected, deployed
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by VARCHAR,                 -- 'user', 'agent:discovery', etc.
    updated_at TIMESTAMP,
    outcome TEXT,                       -- Final conclusion
    confidence_score DECIMAL(3,2)       -- 0.00-1.00, post-validation
);

CREATE TABLE hypothesis_experiments (
    hypothesis_id VARCHAR,
    experiment_id VARCHAR,              -- MLflow run_id
    relationship VARCHAR,               -- 'primary', 'sensitivity', 'robustness'
    PRIMARY KEY (hypothesis_id, experiment_id)
);
```

#### Hypothesis Lifecycle

```
DRAFT ‚Üí TESTING ‚Üí VALIDATED/REJECTED ‚Üí (if validated) ‚Üí DEPLOYED
  ‚îÇ        ‚îÇ              ‚îÇ                                  ‚îÇ
  ‚îÇ        ‚îÇ              ‚îî‚îÄ‚îÄ requires statistical evidence  ‚îÇ
  ‚îÇ        ‚îî‚îÄ‚îÄ linked to experiments                         ‚îÇ
  ‚îî‚îÄ‚îÄ requires thesis + prediction + falsification           ‚îÇ
                                                             ‚îî‚îÄ‚îÄ user approval required
```

#### Example Hypothesis

```yaml
hypothesis_id: HYP-2025-003
title: "12-month momentum predicts 1-month forward returns"
thesis: |
  Stocks with strong 12-month returns (excluding last month) tend to
  continue outperforming over the next month. This is a well-documented
  anomaly attributed to investor underreaction.
testable_prediction: |
  A portfolio long top-decile momentum, rebalanced monthly, will
  outperform SPY by >3% annually (risk-adjusted) over 2010-2023.
falsification_criteria: |
  - Sharpe ratio < SPY Sharpe ratio
  - Alpha not statistically significant (p > 0.05)
  - Performance concentrated in <3 years (not robust)
```

### Backtesting with VectorBT

#### Backtest Configuration

```python
@dataclass
class BacktestConfig:
    # Universe
    symbols: list[str]
    start_date: date
    end_date: date

    # Strategy
    signal_function: Callable      # Returns signal DataFrame
    entry_threshold: float = 0.0   # Signal > threshold ‚Üí enter
    exit_threshold: float = 0.0    # Signal < threshold ‚Üí exit

    # Position sizing
    sizing_method: str = 'signal_scaled'  # 'equal', 'volatility', 'signal_scaled'
    max_position_pct: float = 0.10
    max_positions: int = 20

    # Costs (IBKR realistic)
    commission_pct: float = 0.0005  # 5 bps
    slippage_pct: float = 0.001     # 10 bps

    # Validation
    train_end: date                 # Walk-forward split
    test_start: date
```

#### Standard Metrics

Every backtest computes:

| Metric | Description |
|--------|-------------|
| `total_return` | Cumulative return over period |
| `cagr` | Compound annual growth rate |
| `sharpe_ratio` | Risk-adjusted return (rf=0) |
| `sortino_ratio` | Downside risk-adjusted return |
| `max_drawdown` | Worst peak-to-trough decline |
| `calmar_ratio` | CAGR / max drawdown |
| `win_rate` | % of profitable trades |
| `profit_factor` | Gross profit / gross loss |
| `avg_trade_return` | Mean return per trade |
| `trades_per_year` | Turnover indicator |
| `alpha` | Excess return vs benchmark |
| `beta` | Market sensitivity |
| `information_ratio` | Alpha / tracking error |

#### Benchmark Comparison

Every backtest automatically includes:
- **SPY buy-and-hold** ‚Äî Always computed
- **Style benchmark** ‚Äî If applicable (e.g., IWM for small-cap strategy)

### MLflow Integration

#### Experiment Structure

```
MLflow Tracking Server (local)
‚îú‚îÄ‚îÄ Experiment: "momentum-strategies"
‚îÇ   ‚îú‚îÄ‚îÄ Run: "HYP-2025-003-v1" (params, metrics, artifacts)
‚îÇ   ‚îú‚îÄ‚îÄ Run: "HYP-2025-003-v2-sensitivity"
‚îÇ   ‚îî‚îÄ‚îÄ Run: "HYP-2025-003-v3-robustness"
‚îú‚îÄ‚îÄ Experiment: "mean-reversion"
‚îÇ   ‚îî‚îÄ‚îÄ ...
```

#### What Gets Logged

| Category | Items |
|----------|-------|
| **Parameters** | All BacktestConfig fields, feature versions, universe snapshot |
| **Metrics** | All standard metrics (train and test periods separately) |
| **Artifacts** | Equity curve (PNG), trade log (CSV), signal distribution (PNG) |
| **Tags** | hypothesis_id, run_type (primary/sensitivity/robustness), status |

#### Reproducibility Contract

Any logged run can be exactly reproduced by:
1. Checking out the code version (git SHA logged)
2. Using the logged parameters
3. Using the logged data snapshot (feature versions)

### Audit/Lineage System

The core differentiator ‚Äî every decision is traceable.

#### Lineage Schema

```sql
CREATE TABLE lineage (
    lineage_id INTEGER PRIMARY KEY,
    event_type VARCHAR NOT NULL,    -- 'hypothesis_created', 'experiment_run',
                                    -- 'validation_passed', 'deployment_approved'
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    actor VARCHAR,                  -- 'user', 'agent:discovery', 'system:scheduler'
    hypothesis_id VARCHAR,
    experiment_id VARCHAR,
    details JSONB,                  -- Event-specific metadata
    parent_lineage_id INTEGER       -- For causal chains
);
```

#### Lineage Queries

"Why was this strategy deployed?"
```sql
SELECT * FROM lineage
WHERE hypothesis_id = 'HYP-2025-003'
ORDER BY timestamp;
```

Returns the full chain: hypothesis creation ‚Üí experiments ‚Üí validation ‚Üí approval ‚Üí deployment.

"What experiments did the agent run last week?"
```sql
SELECT * FROM lineage
WHERE actor LIKE 'agent:%'
  AND event_type = 'experiment_run'
  AND timestamp > NOW() - INTERVAL '7 days';
```

### Research Workflow

#### Execution Permissions

| Step | Agent | User | Notes |
|------|-------|------|-------|
| **Hypothesize** | ‚úÖ | ‚úÖ | Agents can propose, users can create directly |
| **Experiment** | ‚úÖ | ‚úÖ | Either can trigger backtests |
| **Analyze** | ‚úÖ | ‚úÖ | Agents surface insights, users review |
| **Conclude** | ‚úÖ | ‚úÖ | Either can update hypothesis status |
| **Deploy** | ‚ùå | ‚úÖ | **User approval required** ‚Äî no autonomous trading |

#### Workflow Steps

```
1. HYPOTHESIZE (agent or user)
   ‚îî‚îÄ‚îÄ Create hypothesis
   ‚îî‚îÄ‚îÄ Define thesis, prediction, falsification
   ‚îî‚îÄ‚îÄ Agent-created hypotheses tagged with source reasoning

2. EXPERIMENT (agent or user)
   ‚îî‚îÄ‚îÄ Run primary backtest
   ‚îî‚îÄ‚îÄ Log to MLflow, link to hypothesis
   ‚îî‚îÄ‚îÄ Agents can batch-run multiple hypotheses

3. ANALYZE (agent or user)
   ‚îî‚îÄ‚îÄ Review metrics vs. falsification criteria
   ‚îî‚îÄ‚îÄ Run sensitivity tests (vary parameters)
   ‚îî‚îÄ‚îÄ Run robustness tests (different periods, universes)
   ‚îî‚îÄ‚îÄ Agents generate summary reports for user review

4. CONCLUDE (agent or user)
   ‚îî‚îÄ‚îÄ Update hypothesis status (validated/rejected)
   ‚îî‚îÄ‚îÄ Document outcome and confidence
   ‚îî‚îÄ‚îÄ If validated: flag for user review

5. DEPLOY (user only)
   ‚îî‚îÄ‚îÄ User reviews validated hypothesis
   ‚îî‚îÄ‚îÄ User explicitly approves for paper trading
   ‚îî‚îÄ‚îÄ Paper trade for N days
   ‚îî‚îÄ‚îÄ User compares live vs. backtest
   ‚îî‚îÄ‚îÄ User promotes to live or rejects
```

#### Agent Guardrails

- Agents **cannot** approve deployment
- Agents **cannot** modify deployed strategies
- Agents **cannot** disable risk limits
- All agent actions logged with full reasoning
- User can pause/disable agent research at any time

---

## Section 5: Control Layer (Dashboard + Agents)

### Overview

The Control Layer provides all interfaces to the platform: a Streamlit dashboard for visual interaction, MCP servers for Claude integration, and scheduled agents for autonomous research. All share the same Platform API, ensuring consistency and auditability.

### Platform API

The single interface that all consumers use. No direct database or VectorBT access from UI or agents.

#### API Structure

```python
# hrp/api/platform.py

class PlatformAPI:
    """Single entry point for all platform operations."""

    # Data operations
    def get_prices(self, symbols: list[str], start: date, end: date) -> pd.DataFrame
    def get_features(self, symbols: list[str], features: list[str], date: date) -> pd.DataFrame
    def get_universe(self, date: date) -> list[str]

    # Hypothesis operations
    def create_hypothesis(self, title: str, thesis: str, prediction: str,
                          falsification: str, actor: str) -> str  # returns hypothesis_id
    def update_hypothesis(self, hypothesis_id: str, status: str, outcome: str = None)
    def list_hypotheses(self, status: str = None) -> list[dict]
    def get_hypothesis(self, hypothesis_id: str) -> dict

    # Experiment operations
    def run_backtest(self, config: BacktestConfig, hypothesis_id: str = None,
                     actor: str = 'user') -> str  # returns experiment_id
    def get_experiment(self, experiment_id: str) -> dict
    def compare_experiments(self, experiment_ids: list[str]) -> pd.DataFrame

    # Deployment operations (user-only enforced at this layer)
    def approve_deployment(self, hypothesis_id: str, actor: str) -> bool
    def get_deployed_strategies(self) -> list[dict]

    # Lineage operations
    def get_lineage(self, hypothesis_id: str = None, experiment_id: str = None) -> list[dict]
    def log_event(self, event_type: str, actor: str, details: dict)
```

#### Actor Validation

```python
def approve_deployment(self, hypothesis_id: str, actor: str) -> bool:
    if actor.startswith('agent:'):
        raise PermissionError("Deployment requires user approval")
    # ... proceed with deployment
```

### Streamlit Dashboard

#### Pages

| Page | Purpose |
|------|---------|
| **Home** | System status, recent activity, alerts |
| **Data Health** | Ingestion status, data quality, completeness |
| **Universe** | Current tradeable symbols, exclusions, history |
| **Hypotheses** | Browse, create, update research hypotheses |
| **Experiments** | MLflow integration, compare runs, view artifacts |
| **Backtest** | Run new backtests, configure parameters |
| **Deployment** | Review validated strategies, approve/reject |
| **Lineage** | Trace decisions, audit trail explorer |
| **Agents** | Agent status, recent actions, pause/resume |
| **Settings** | API keys, notification preferences, risk limits |

#### Key Interactions

```
Dashboard                    Platform API                 Backend
    ‚îÇ                            ‚îÇ                           ‚îÇ
    ‚îÇ‚îÄ‚îÄ[Create Hypothesis]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ                           ‚îÇ
    ‚îÇ                            ‚îÇ‚îÄ‚îÄ[Insert + Log Event]‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ
    ‚îÇ‚óÑ‚îÄ[hypothesis_id]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ                           ‚îÇ
    ‚îÇ                            ‚îÇ                           ‚îÇ
    ‚îÇ‚îÄ‚îÄ[Run Backtest]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ                           ‚îÇ
    ‚îÇ                            ‚îÇ‚îÄ‚îÄ[VectorBT + MLflow]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ
    ‚îÇ‚óÑ‚îÄ[experiment_id]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ                           ‚îÇ
    ‚îÇ                            ‚îÇ                           ‚îÇ
    ‚îÇ‚îÄ‚îÄ[Approve Deployment]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ                           ‚îÇ
    ‚îÇ                            ‚îÇ‚îÄ‚îÄ[Validate user actor]‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ
    ‚îÇ                            ‚îÇ‚îÄ‚îÄ[Update status + Log]‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ
    ‚îÇ‚óÑ‚îÄ[Success]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ                           ‚îÇ
```

### MCP Servers (Claude Integration)

MCP (Model Context Protocol) servers expose Platform API operations as tools that Claude can call.

#### Server Structure

```python
# hrp/mcp/research_server.py
from fastmcp import FastMCP

mcp = FastMCP("HRP Research")

@mcp.tool()
def create_hypothesis(title: str, thesis: str, prediction: str,
                      falsification: str) -> str:
    """Create a new research hypothesis."""
    api = PlatformAPI()
    return api.create_hypothesis(
        title=title, thesis=thesis, prediction=prediction,
        falsification=falsification, actor='agent:claude-interactive'
    )

@mcp.tool()
def run_backtest(hypothesis_id: str, symbols: list[str],
                 start_date: str, end_date: str,
                 signal_type: str = 'momentum_20d') -> dict:
    """Run a backtest for a hypothesis."""
    api = PlatformAPI()
    config = BacktestConfig(...)
    experiment_id = api.run_backtest(config, hypothesis_id,
                                      actor='agent:claude-interactive')
    return api.get_experiment(experiment_id)

@mcp.tool()
def analyze_results(experiment_id: str) -> str:
    """Get analysis summary for an experiment."""
    api = PlatformAPI()
    exp = api.get_experiment(experiment_id)
    # Format metrics, compare to benchmark, assess vs falsification criteria
    return format_analysis(exp)

# Note: No deploy tool exposed to agents
```

#### Available MCP Tools

| Tool | Description | Agent Access |
|------|-------------|--------------|
| `list_hypotheses` | Browse existing research | ‚úÖ |
| `create_hypothesis` | Propose new research | ‚úÖ |
| `get_universe` | Current tradeable symbols | ‚úÖ |
| `get_features` | Retrieve feature data | ‚úÖ |
| `run_backtest` | Execute backtest | ‚úÖ |
| `analyze_results` | Get experiment analysis | ‚úÖ |
| `compare_strategies` | Compare multiple experiments | ‚úÖ |
| `update_hypothesis` | Change status, add notes | ‚úÖ |
| `approve_deployment` | Deploy to paper trading | ‚ùå (not exposed) |

#### Claude Code Integration

```json
// ~/.claude/claude_desktop_config.json (or similar)
{
  "mcpServers": {
    "hrp-research": {
      "command": "python",
      "args": ["-m", "hrp.mcp.research_server"],
      "env": {
        "HRP_DB_PATH": "/path/to/hrp.duckdb"
      }
    }
  }
}
```

### Scheduled Agents

Background agents run autonomously on schedules, performing research tasks without user interaction.

#### Agent Types

| Agent | Schedule | Purpose |
|-------|----------|---------|
| **Data Monitor** | Every 6 hours | Check data freshness, alert on issues |
| **Discovery Agent** | Daily (overnight) | Scan for new hypothesis ideas from literature, patterns |
| **Validation Agent** | Daily | Run robustness checks on validated hypotheses |
| **Report Agent** | Weekly (Sunday) | Generate weekly research summary email |

#### Agent Implementation

```python
# hrp/agents/discovery_agent.py

class DiscoveryAgent:
    def __init__(self):
        self.api = PlatformAPI()
        self.claude = Anthropic()

    def run(self):
        """Discover new research hypotheses."""
        # Get current market context
        universe = self.api.get_universe(date.today())
        recent_features = self.api.get_features(universe, ALL_FEATURES, date.today())

        # Ask Claude to identify patterns worth investigating
        prompt = f"""
        Review this market data and suggest 3 research hypotheses
        worth testing. For each, provide:
        - Title
        - Thesis (why this might work)
        - Testable prediction
        - Falsification criteria

        Current universe: {len(universe)} stocks
        Feature summary: {summarize(recent_features)}
        """

        response = self.claude.messages.create(
            model="claude-sonnet-4-20250514",
            messages=[{"role": "user", "content": prompt}]
        )

        # Parse and create hypotheses
        hypotheses = parse_hypotheses(response.content)
        for h in hypotheses:
            self.api.create_hypothesis(
                **h, actor='agent:discovery'
            )

        # Log completion
        self.api.log_event('agent_run_complete', 'agent:discovery',
                          {'hypotheses_created': len(hypotheses)})
```

#### Scheduler

```python
# hrp/agents/scheduler.py
# Uses APScheduler or simple cron

from apscheduler.schedulers.background import BackgroundScheduler

scheduler = BackgroundScheduler()

scheduler.add_job(DataMonitorAgent().run, 'interval', hours=6)
scheduler.add_job(DiscoveryAgent().run, 'cron', hour=2)  # 2 AM
scheduler.add_job(ValidationAgent().run, 'cron', hour=3)  # 3 AM
scheduler.add_job(ReportAgent().run, 'cron', day_of_week='sun', hour=8)

scheduler.start()
```

### Notifications

Simple email-based notifications for important events.

#### Notification Events

| Event | Priority | Notification |
|-------|----------|--------------|
| Data ingestion failed | High | Immediate email |
| Hypothesis validated | Medium | Daily digest |
| Agent error | High | Immediate email |
| Weekly report ready | Low | Weekly email |
| Deployed strategy underperforming | High | Immediate email |

#### Implementation

```python
# hrp/notifications/email.py
import resend  # or smtplib for Gmail

def send_alert(subject: str, body: str, priority: str = 'medium'):
    if priority == 'high':
        resend.Emails.send({
            "from": "hrp@yourdomain.com",
            "to": "you@email.com",
            "subject": f"[HRP Alert] {subject}",
            "text": body
        })
    else:
        # Queue for digest
        queue_for_digest(subject, body)
```

### Remote Access

Access the dashboard from anywhere using Tailscale.

#### Setup

1. Install Tailscale on Mac Mini
2. Install Tailscale on mobile/laptop
3. Access dashboard at `http://mac-mini:8501` from any Tailscale-connected device

No port forwarding, no public exposure, encrypted tunnel.

---

## Section 6: ML Experiment Framework

### Overview

ML in trading is a minefield of overfitting. This framework prioritizes rigorous validation over model complexity, using MLflow to track everything and enforcing discipline that prevents fooling yourself.

### Hardware Constraints

M4 Mac Mini with 16GB RAM limits what's practical:

| Approach | Feasibility | Notes |
|----------|-------------|-------|
| Gradient boosting (XGBoost, LightGBM) | ‚úÖ Excellent | Fast, memory-efficient, strong baseline |
| Random forests | ‚úÖ Good | Moderate memory, parallelizes well |
| Linear models (Ridge, Lasso, ElasticNet) | ‚úÖ Excellent | Fast, interpretable |
| Small neural networks | ‚ö†Ô∏è Limited | MPS acceleration helps, but keep small |
| Deep learning (transformers, LSTMs) | ‚ùå Not recommended | Memory and training time constraints |
| Large hyperparameter sweeps | ‚ö†Ô∏è Limited | Run overnight, constrain search space |

**Recommendation:** Start with gradient boosting (LightGBM) and linear models. These are often competitive with deep learning for tabular financial data and train in seconds/minutes, not hours.

### Model Types

#### Supported Models

```python
# hrp/ml/models.py

SUPPORTED_MODELS = {
    # Linear
    'ridge': Ridge,
    'lasso': Lasso,
    'elastic_net': ElasticNet,

    # Tree-based
    'lightgbm': LGBMRegressor,
    'xgboost': XGBRegressor,
    'random_forest': RandomForestRegressor,

    # Simple neural net (optional)
    'mlp': MLPRegressor,  # sklearn, CPU-friendly
}
```

#### Model Configuration

```python
@dataclass
class MLConfig:
    model_type: str
    target: str                    # 'return_5d', 'return_20d', etc.
    features: list[str]            # Feature names from feature store

    # Training
    train_start: date
    train_end: date
    validation_start: date
    validation_end: date
    test_start: date
    test_end: date

    # Model-specific hyperparameters
    hyperparameters: dict

    # Regularization
    feature_selection: bool = True
    max_features: int = 20         # Prevent overfitting with too many features
```

### Feature Engineering

#### Feature Categories

| Category | Examples | Notes |
|----------|----------|-------|
| **Momentum** | return_20d, return_60d, return_252d | Core signals |
| **Volatility** | volatility_20d, volatility_60d | Risk scaling |
| **Volume** | volume_ratio, volume_trend | Liquidity/interest |
| **Technical** | rsi_14, price_to_sma_50, price_to_sma_200 | Price patterns |
| **Cross-sectional** | momentum_rank, volatility_percentile | Relative positioning |

#### Feature Selection

Automatic feature selection to prevent overfitting:

```python
def select_features(X_train, y_train, max_features=20):
    """Select top features using mutual information."""
    mi_scores = mutual_info_regression(X_train, y_train)
    top_indices = np.argsort(mi_scores)[-max_features:]
    return X_train.columns[top_indices].tolist()
```

### Training Pipeline

```
1. LOAD DATA
   ‚îî‚îÄ‚îÄ Pull features and targets from feature store
   ‚îî‚îÄ‚îÄ Align dates, handle missing values

2. SPLIT (temporal, no shuffle)
   ‚îî‚îÄ‚îÄ Train: 2010-2018
   ‚îî‚îÄ‚îÄ Validation: 2018-2020 (hyperparameter tuning)
   ‚îî‚îÄ‚îÄ Test: 2020-2023 (final evaluation, touch once)

3. FEATURE SELECTION
   ‚îî‚îÄ‚îÄ Select on train set only
   ‚îî‚îÄ‚îÄ Apply same features to validation/test

4. TRAIN
   ‚îî‚îÄ‚îÄ Fit model on train set
   ‚îî‚îÄ‚îÄ Log parameters to MLflow

5. VALIDATE
   ‚îî‚îÄ‚îÄ Tune hyperparameters on validation set
   ‚îî‚îÄ‚îÄ Use Optuna or simple grid search
   ‚îî‚îÄ‚îÄ Log all trials to MLflow

6. EVALUATE
   ‚îî‚îÄ‚îÄ Final evaluation on test set
   ‚îî‚îÄ‚îÄ Compare to baseline (buy-and-hold, simple momentum)
   ‚îî‚îÄ‚îÄ Log metrics and artifacts to MLflow

7. BACKTEST
   ‚îî‚îÄ‚îÄ Convert predictions to trading signals
   ‚îî‚îÄ‚îÄ Run through VectorBT with realistic costs
   ‚îî‚îÄ‚îÄ This is the real test ‚Äî prediction accuracy ‚â† profitability
```

### Walk-Forward Validation

For production-ready strategies, use expanding or rolling walk-forward:

```python
def walk_forward_train(config: MLConfig, window_type='expanding'):
    """
    Train and evaluate using walk-forward methodology.

    Expanding: Train on all data up to t, predict t+1
    Rolling: Train on fixed window ending at t, predict t+1
    """
    results = []

    for train_end, test_start, test_end in generate_folds(config):
        # Train
        model = train_model(config, train_end=train_end)

        # Predict
        predictions = model.predict(get_features(test_start, test_end))

        # Evaluate
        fold_metrics = evaluate(predictions, actuals)
        results.append(fold_metrics)

        # Log to MLflow
        mlflow.log_metrics(fold_metrics, step=fold_number)

    return aggregate_results(results)
```

### MLflow Integration

#### Experiment Naming

```
MLflow Experiments
‚îú‚îÄ‚îÄ ml-momentum-prediction        # ML models predicting momentum-based returns
‚îú‚îÄ‚îÄ ml-mean-reversion            # ML models for mean reversion signals
‚îú‚îÄ‚îÄ ml-feature-research          # Feature importance and selection experiments
‚îî‚îÄ‚îÄ ml-ensemble                  # Combining multiple models
```

#### What Gets Logged

| Category | Items |
|----------|-------|
| **Parameters** | model_type, hyperparameters, features used, train/val/test dates |
| **Metrics** | MSE, MAE, R¬≤, IC (information coefficient), hit_rate, backtest_sharpe |
| **Artifacts** | Feature importance plot, prediction vs actual scatter, equity curve |
| **Tags** | hypothesis_id, walk_forward_fold, model_version |

#### Model Registry

Validated models are registered for deployment:

```python
# Only register if passes validation criteria
if metrics['test_sharpe'] > 0.5 and metrics['test_ic'] > 0.03:
    mlflow.register_model(
        f"runs:/{run_id}/model",
        f"strategy-{hypothesis_id}"
    )
```

### Preventing Overfitting

#### Mandatory Checks

| Check | Threshold | Action if Failed |
|-------|-----------|------------------|
| Train vs Test Sharpe ratio | Test < 0.5 √ó Train | Flag as overfit |
| Feature count | > 30 features | Require justification |
| Hyperparameter trials | > 100 trials | Require justification |
| Test set touched | > 3 times per hypothesis | Lock test set |
| Walk-forward consistency | Std(fold_sharpe) > mean | Flag as unstable |

#### Test Set Discipline

```python
class TestSetGuard:
    """Prevent excessive test set peeking."""

    def __init__(self, hypothesis_id: str):
        self.hypothesis_id = hypothesis_id
        self.test_evaluations = self._load_count()

    def evaluate_on_test(self, model, test_data):
        if self.test_evaluations >= 3:
            raise ValueError(
                f"Test set accessed {self.test_evaluations} times for "
                f"{self.hypothesis_id}. Further access requires explicit override."
            )
        self.test_evaluations += 1
        self._save_count()
        return model.evaluate(test_data)
```

### Signal Generation

Converting ML predictions to trading signals:

```python
def predictions_to_signals(predictions: pd.DataFrame,
                           method: str = 'rank') -> pd.DataFrame:
    """
    Convert raw predictions to actionable signals.

    Methods:
    - 'rank': Cross-sectional rank, go long top decile
    - 'threshold': Go long if prediction > threshold
    - 'zscore': Normalize predictions, signal = zscore
    """
    if method == 'rank':
        # Top 10% get signal = 1, rest = 0
        ranks = predictions.rank(pct=True, axis=1)
        return (ranks > 0.9).astype(float)
    elif method == 'threshold':
        return (predictions > threshold).astype(float)
    elif method == 'zscore':
        return (predictions - predictions.mean()) / predictions.std()
```

---

## Section 7: Risk & Validation Framework

### Overview

Two distinct concerns: **Risk Management** (protecting capital during trading) and **Validation** (ensuring strategies are statistically sound before deployment). Both are enforced by the platform, not left to human discipline.

---

## Part A: Risk Management

### Position Limits

Configurable per strategy, with hard platform-wide floors.

```python
@dataclass
class RiskLimits:
    # Position sizing
    max_position_pct: float = 0.10      # Max 10% in single position
    max_positions: int = 20             # Max simultaneous positions
    min_position_pct: float = 0.02      # Min 2% (avoid tiny positions)

    # Portfolio-level
    max_gross_exposure: float = 1.0     # 100% (no leverage for now)
    max_sector_exposure: float = 0.30   # Max 30% in one sector

    # Drawdown
    strategy_stop_loss: float = 0.15    # Pause strategy at 15% drawdown
    portfolio_stop_loss: float = 0.20   # Pause all trading at 20% drawdown
```

#### Hard Floors (Platform-Enforced)

These cannot be overridden, even by user:

| Limit | Value | Rationale |
|-------|-------|-----------|
| Max single position | 25% | Prevent catastrophic single-stock risk |
| Max gross exposure | 150% | Limit leverage |
| Min positions for deployment | 5 | Ensure diversification |

### Drawdown Controls

```python
class DrawdownMonitor:
    """Monitor and enforce drawdown limits."""

    def __init__(self, limits: RiskLimits):
        self.limits = limits
        self.high_water_mark = None
        self.current_drawdown = 0.0

    def update(self, portfolio_value: float) -> str:
        if self.high_water_mark is None:
            self.high_water_mark = portfolio_value

        self.high_water_mark = max(self.high_water_mark, portfolio_value)
        self.current_drawdown = (self.high_water_mark - portfolio_value) / self.high_water_mark

        if self.current_drawdown >= self.limits.portfolio_stop_loss:
            return 'HALT_ALL'  # Stop all trading
        elif self.current_drawdown >= self.limits.strategy_stop_loss:
            return 'HALT_STRATEGY'  # Stop this strategy
        else:
            return 'OK'
```

### Transaction Cost Model

Realistic IBKR costs for backtesting and live trading.

```python
@dataclass
class CostModel:
    # Commission (IBKR tiered)
    commission_per_share: float = 0.005  # $0.005/share
    commission_min: float = 1.00         # $1 minimum
    commission_max_pct: float = 0.01     # 1% of trade value max

    # Spread (estimated from historical data)
    spread_bps: float = 5                # 5 bps half-spread for liquid large-cap

    # Slippage (market impact)
    slippage_bps: float = 5              # 5 bps for small orders in liquid names

    def total_cost_bps(self, trade_value: float, shares: int) -> float:
        commission = max(self.commission_min,
                        min(shares * self.commission_per_share,
                            trade_value * self.commission_max_pct))
        commission_bps = (commission / trade_value) * 10000
        return commission_bps + self.spread_bps + self.slippage_bps
```

### Risk Dashboard Metrics

Real-time monitoring in Streamlit:

| Metric | Description | Alert Threshold |
|--------|-------------|-----------------|
| Current drawdown | From high-water mark | > 10% |
| Daily VaR (95%) | Expected daily loss | > 2% |
| Gross exposure | Total position value / NAV | > 95% |
| Position concentration | Largest position % | > 15% |
| Sector concentration | Largest sector % | > 25% |
| Correlation to SPY | Rolling 20-day | > 0.95 (too passive) |

---

## Part B: Statistical Validation

### Validation Requirements

A hypothesis cannot be marked "validated" without passing these checks.

#### Minimum Criteria

| Criterion | Threshold | Rationale |
|-----------|-----------|-----------|
| Out-of-sample Sharpe | > 0.5 | Minimum risk-adjusted return |
| Out-of-sample period | ‚â• 2 years | Sufficient data for significance |
| Number of trades | ‚â• 100 | Statistical power |
| Profit factor | > 1.2 | Wins must exceed losses meaningfully |
| Max drawdown | < 25% | Survivable drawdown |
| Win rate | > 40% | Not purely dependent on few big wins |

#### Statistical Significance

```python
def test_significance(strategy_returns: pd.Series,
                      benchmark_returns: pd.Series,
                      alpha: float = 0.05) -> dict:
    """
    Test if strategy significantly outperforms benchmark.
    """
    excess_returns = strategy_returns - benchmark_returns

    # T-test for mean excess return > 0
    t_stat, p_value = ttest_1samp(excess_returns, 0)
    p_value_one_sided = p_value / 2 if t_stat > 0 else 1 - p_value / 2

    # Bootstrap confidence interval for Sharpe
    sharpe_ci = bootstrap_sharpe_ci(strategy_returns, confidence=0.95)

    return {
        'excess_return_annualized': excess_returns.mean() * 252,
        't_statistic': t_stat,
        'p_value': p_value_one_sided,
        'significant': p_value_one_sided < alpha,
        'sharpe_95_ci_lower': sharpe_ci[0],
        'sharpe_95_ci_upper': sharpe_ci[1],
    }
```

### Multiple Hypothesis Correction

When testing many strategies, some will appear profitable by chance. Apply corrections.

#### Bonferroni Correction (Conservative)

```python
def bonferroni_threshold(base_alpha: float, num_hypotheses: int) -> float:
    """Adjusted significance threshold."""
    return base_alpha / num_hypotheses

# Example: Testing 20 hypotheses at Œ±=0.05
# Adjusted threshold: 0.05 / 20 = 0.0025
# Strategy must have p-value < 0.0025 to be significant
```

#### False Discovery Rate (Less Conservative)

```python
def benjamini_hochberg(p_values: list[float], alpha: float = 0.05) -> list[bool]:
    """
    Control false discovery rate at alpha level.
    Returns which hypotheses to reject (True = significant).
    """
    n = len(p_values)
    sorted_indices = np.argsort(p_values)
    sorted_pvals = np.array(p_values)[sorted_indices]

    # Find largest k where p(k) <= k/n * alpha
    thresholds = [(i + 1) / n * alpha for i in range(n)]
    rejected = sorted_pvals <= thresholds

    # Find cutoff
    if rejected.any():
        max_rejected = np.max(np.where(rejected)[0])
        rejected = np.zeros(n, dtype=bool)
        rejected[:max_rejected + 1] = True

    # Map back to original order
    result = np.zeros(n, dtype=bool)
    result[sorted_indices] = rejected
    return result.tolist()
```

### Robustness Checks

Required before validation:

| Check | Method | Pass Criteria |
|-------|--------|---------------|
| **Parameter sensitivity** | Vary key parameters ¬±20% | Sharpe stays > 0.3 |
| **Time period stability** | Test on 3+ subperiods | Profitable in > 2/3 |
| **Universe stability** | Test on different subsets | Performance consistent |
| **Regime robustness** | Test in bull/bear/sideways | Not purely bull-market strategy |
| **Out-of-sample decay** | Compare IS vs OOS Sharpe | OOS > 50% of IS |

```python
def robustness_report(hypothesis_id: str) -> dict:
    """Generate comprehensive robustness analysis."""
    experiments = get_experiments_for_hypothesis(hypothesis_id)

    return {
        'parameter_sensitivity': analyze_parameter_sensitivity(experiments),
        'time_stability': analyze_time_periods(experiments),
        'regime_analysis': analyze_market_regimes(experiments),
        'oos_decay': calculate_oos_decay(experiments),
        'overall_pass': all_checks_pass(...)
    }
```

### Validation Workflow

```
1. PRIMARY BACKTEST
   ‚îî‚îÄ‚îÄ Run on train+validation period
   ‚îî‚îÄ‚îÄ Must meet minimum criteria

2. STATISTICAL TESTS
   ‚îî‚îÄ‚îÄ Significance test vs benchmark
   ‚îî‚îÄ‚îÄ Apply multiple hypothesis correction if many strategies tested

3. ROBUSTNESS CHECKS
   ‚îî‚îÄ‚îÄ Parameter sensitivity
   ‚îî‚îÄ‚îÄ Time period stability
   ‚îî‚îÄ‚îÄ Regime analysis

4. FINAL OUT-OF-SAMPLE
   ‚îî‚îÄ‚îÄ Test set evaluation (guarded, max 3 touches)
   ‚îî‚îÄ‚îÄ Compare to in-sample performance
   ‚îî‚îÄ‚îÄ Document decay ratio

5. VALIDATION DECISION
   ‚îî‚îÄ‚îÄ All checks pass ‚Üí Status: VALIDATED
   ‚îî‚îÄ‚îÄ Any check fails ‚Üí Status: REJECTED (with reason)
   ‚îî‚îÄ‚îÄ Edge cases ‚Üí Status: NEEDS_REVIEW (user decision)
```

### Validation Report Template

Generated automatically for each validated hypothesis:

```markdown
# Validation Report: HYP-2025-003

## Summary
- **Status:** VALIDATED
- **Confidence Score:** 0.72
- **Validated Date:** 2025-01-15
- **Validated By:** system (auto)

## Performance Metrics (Out-of-Sample: 2021-2023)
| Metric | Value | Threshold | Pass |
|--------|-------|-----------|------|
| Sharpe Ratio | 0.83 | > 0.5 | ‚úÖ |
| CAGR | 12.4% | ‚Äî | ‚Äî |
| Max Drawdown | 18.2% | < 25% | ‚úÖ |
| Win Rate | 54% | > 40% | ‚úÖ |
| Profit Factor | 1.45 | > 1.2 | ‚úÖ |
| Trade Count | 847 | ‚â• 100 | ‚úÖ |

## Statistical Significance
- Excess return vs SPY: 4.2% annualized
- t-statistic: 2.34
- p-value: 0.0098
- **Significant at Œ±=0.05:** ‚úÖ

## Robustness
| Check | Result |
|-------|--------|
| Parameter sensitivity | PASS |
| Time period stability | PASS (3/3 subperiods) |
| Regime analysis | PASS (profitable in 2/3 regimes) |
| OOS decay | 0.62 (acceptable) |

## Recommendation
Approved for paper trading. Monitor for 30 days before live deployment consideration.
```

---

## Section 8: Technology Stack

### Overview

Lean stack prioritizing mature, well-maintained tools. Avoid bleeding-edge dependencies. Everything runs locally on macOS.

### Core Stack

| Layer | Tool | Version | Purpose |
|-------|------|---------|---------|
| **Language** | Python | 3.11+ | Primary language |
| **Database** | DuckDB | 1.0+ | Analytical data storage |
| **Backtesting** | VectorBT | 0.26+ | Vectorized backtesting engine |
| **ML Tracking** | MLflow | 2.10+ | Experiment tracking, model registry |
| **Dashboard** | Streamlit | 1.30+ | Web UI |
| **Agent Protocol** | FastMCP | 0.1+ | Claude MCP server |
| **Scheduling** | APScheduler | 3.10+ | Background job scheduling |

### Python Dependencies

#### Core

```txt
# requirements.txt

# Data
duckdb>=1.0.0
pandas>=2.0.0
numpy>=1.24.0
pyarrow>=14.0.0

# Backtesting
vectorbt>=0.26.0

# ML
scikit-learn>=1.3.0
lightgbm>=4.0.0
xgboost>=2.0.0
optuna>=3.4.0           # Hyperparameter tuning

# Experiment tracking
mlflow>=2.10.0

# Dashboard
streamlit>=1.30.0
plotly>=5.18.0

# Agent integration
fastmcp>=0.1.0
anthropic>=0.18.0       # Claude API

# Scheduling & notifications
apscheduler>=3.10.0
resend>=0.7.0           # Email notifications

# Utilities
python-dotenv>=1.0.0
pydantic>=2.5.0
loguru>=0.7.0           # Logging
httpx>=0.26.0           # HTTP client for data APIs
```

#### Data Sources

```txt
# Data provider SDKs
yfinance>=0.2.30        # Free Yahoo Finance data
polygon-api-client>=1.12.0  # Polygon.io (if subscribed)
alpaca-py>=0.13.0       # Alpaca Markets
```

#### Development

```txt
# requirements-dev.txt
pytest>=7.4.0
pytest-cov>=4.1.0
black>=23.12.0
ruff>=0.1.0
mypy>=1.8.0
pre-commit>=3.6.0
```

### Version Pinning Strategy

```txt
# Use >= for flexibility during development
# Pin exact versions in production lockfile (requirements.lock)

# Generate lockfile:
pip freeze > requirements.lock
```

### System Dependencies

| Dependency | Installation | Purpose |
|------------|--------------|---------|
| Python 3.11+ | `brew install python@3.11` | Runtime |
| Git | Pre-installed on macOS | Version control |
| Tailscale | `brew install tailscale` | Remote access |

### Development Environment

#### Recommended Setup

```bash
# Create virtual environment
python3.11 -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
pip install -r requirements-dev.txt

# Install pre-commit hooks
pre-commit install
```

#### IDE Configuration

VSCode recommended with extensions:
- Python (Microsoft)
- Pylance
- Black Formatter
- Ruff

```json
// .vscode/settings.json
{
    "python.defaultInterpreterPath": ".venv/bin/python",
    "python.formatting.provider": "black",
    "editor.formatOnSave": true,
    "[python]": {
        "editor.defaultFormatter": "ms-python.black-formatter"
    }
}
```

### Services & Ports

Local services when running:

| Service | Port | Command |
|---------|------|---------|
| Streamlit Dashboard | 8501 | `streamlit run hrp/dashboard/app.py` |
| MLflow UI | 5000 | `mlflow ui --backend-store-uri sqlite:///$HOME/hrp-data/mlflow/mlflow.db` |
| MCP Server | stdio | Launched by Claude Code |

### Data Storage Locations

```
~/hrp-data/                    # Data directory (outside repo)
‚îú‚îÄ‚îÄ hrp.duckdb                 # Main database
‚îú‚îÄ‚îÄ mlflow/                    # MLflow artifacts
‚îÇ   ‚îú‚îÄ‚îÄ mlruns/
‚îÇ   ‚îî‚îÄ‚îÄ mlflow.db
‚îú‚îÄ‚îÄ backups/                   # Database backups
‚îî‚îÄ‚îÄ cache/                     # API response cache
```

### Backup Strategy

```bash
# Daily backup script (cron)
#!/bin/bash
BACKUP_DIR=~/hrp-data/backups
DATE=$(date +%Y%m%d)

# Backup DuckDB
cp ~/hrp-data/hrp.duckdb "$BACKUP_DIR/hrp-$DATE.duckdb"

# Backup MLflow
tar -czf "$BACKUP_DIR/mlflow-$DATE.tar.gz" ~/hrp-data/mlflow/

# Keep last 30 days
find "$BACKUP_DIR" -type f -mtime +30 -delete
```

### Hardware Utilization

M4 Mac Mini optimization:

| Resource | Usage |
|----------|-------|
| **CPU** | VectorBT backtests (vectorized), LightGBM training |
| **Memory** | DuckDB queries, pandas DataFrames (monitor with `htop`) |
| **MPS (GPU)** | Optional PyTorch/MLP training acceleration |
| **SSD** | DuckDB storage, MLflow artifacts |

#### Memory Management Tips

```python
# Use DuckDB for large queries instead of loading to pandas
con = duckdb.connect('hrp.duckdb')
result = con.execute("""
    SELECT symbol, date, close
    FROM prices
    WHERE date > '2020-01-01'
""").df()  # Only materialize what you need

# VectorBT memory optimization
vbt.settings.array_wrapper['freq'] = 'D'  # Daily frequency
vbt.settings.caching['enabled'] = False   # Disable caching for large backtests
```

---

## Section 9: Data Sources

### Overview

Budget: ~$100/month. Priority: reliable price data first, fundamentals second. Start with free sources, upgrade as needed.

### Data Source Comparison

#### Price Data

| Source | Cost | History | Quality | API Limits | Recommendation |
|--------|------|---------|---------|------------|----------------|
| **Yahoo Finance** | Free | 20+ years | Good* | Unofficial, may break | Development/backup |
| **Polygon.io** | $29/mo (Basic) | 5+ years | Excellent | 5 calls/min (Basic) | ‚úÖ Primary |
| **Alpaca** | Free | 5+ years | Good | Generous | Good alternative |
| **Tiingo** | $30/mo | 20+ years | Excellent | 500/hour | Good alternative |

*Yahoo Finance: Adjusted prices may have issues; unofficial API can break without notice.

#### Fundamental Data

| Source | Cost | Coverage | Point-in-Time | Recommendation |
|--------|------|----------|---------------|----------------|
| **Yahoo Finance** | Free | Basic ratios | ‚ùå No | Basic use only |
| **Polygon.io** | $79/mo (Stocks Starter) | Full financials | ‚úÖ Yes | ‚úÖ If budget allows |
| **Tiingo** | $30/mo | Good coverage | ‚ö†Ô∏è Partial | Alternative |
| **Alpha Vantage** | $50/mo | Fundamentals | ‚ùå No | Not recommended |
| **SimFin** | Free/$15mo | Good | ‚úÖ Yes | Budget option |

#### Reference Data

| Data | Source | Notes |
|------|--------|-------|
| S&P 500 constituents | Wikipedia / GitHub lists | Update monthly |
| Sector classifications | Yahoo Finance / Polygon | GICS sectors |
| Market holidays | `exchange_calendars` package | NYSE calendar |

### Recommended Setup

#### Tier 1: Free Start (~$0/month)

For initial development and learning:

```python
# Primary: Yahoo Finance (free, unofficial)
import yfinance as yf

# Backup: Alpaca (free, official)
from alpaca.data import StockHistoricalDataClient
```

**Limitations:**
- Yahoo may break without notice
- No point-in-time fundamentals
- Survivorship bias in historical constituents

#### Tier 2: Serious Research (~$30-60/month)

```python
# Price data: Polygon.io Basic ($29/mo)
from polygon import RESTClient

# OR Tiingo ($30/mo) - better historical depth
import requests  # Tiingo has simple REST API
```

**Unlocks:**
- Reliable, official API
- Better data quality
- Proper corporate actions

#### Tier 3: Full Research (~$80-110/month)

```python
# Polygon.io Stocks Starter ($79/mo)
# Includes fundamentals with point-in-time

# Plus: SimFin for backup fundamentals ($15/mo or free tier)
```

**Unlocks:**
- Point-in-time fundamentals
- Proper backtesting of fundamental strategies

### API Configuration

#### Environment Variables

```bash
# .env file (never commit to git)
POLYGON_API_KEY=your_key_here
ALPACA_API_KEY=your_key_here
ALPACA_SECRET_KEY=your_secret_here
TIINGO_API_KEY=your_key_here
```

#### Polygon.io Setup

```python
# hrp/data/sources/polygon_source.py
from polygon import RESTClient
from datetime import date
import os

class PolygonSource:
    def __init__(self):
        self.client = RESTClient(os.getenv('POLYGON_API_KEY'))

    def get_daily_bars(self, symbol: str, start: date, end: date) -> pd.DataFrame:
        """Fetch daily OHLCV data."""
        bars = self.client.get_aggs(
            ticker=symbol,
            multiplier=1,
            timespan="day",
            from_=start.isoformat(),
            to=end.isoformat(),
            limit=50000
        )

        df = pd.DataFrame([{
            'date': pd.to_datetime(b.timestamp, unit='ms').date(),
            'open': b.open,
            'high': b.high,
            'low': b.low,
            'close': b.close,
            'volume': b.volume,
            'vwap': b.vwap,
        } for b in bars])

        return df

    def get_splits(self, symbol: str) -> pd.DataFrame:
        """Fetch stock splits for adjustment."""
        splits = self.client.list_splits(ticker=symbol)
        return pd.DataFrame([{
            'date': s.execution_date,
            'split_ratio': s.split_to / s.split_from
        } for s in splits])
```

#### Yahoo Finance Setup (Free Backup)

```python
# hrp/data/sources/yfinance_source.py
import yfinance as yf

class YFinanceSource:
    def get_daily_bars(self, symbol: str, start: date, end: date) -> pd.DataFrame:
        """Fetch daily OHLCV data from Yahoo Finance."""
        ticker = yf.Ticker(symbol)
        df = ticker.history(start=start, end=end, auto_adjust=False)

        return df.reset_index().rename(columns={
            'Date': 'date',
            'Open': 'open',
            'High': 'high',
            'Low': 'low',
            'Close': 'close',
            'Adj Close': 'adj_close',
            'Volume': 'volume'
        })
```

### Data Quality Considerations

#### Known Issues by Source

| Source | Issue | Mitigation |
|--------|-------|------------|
| Yahoo Finance | Adjusted prices recalculated retroactively | Store raw + adjustment factors separately |
| Yahoo Finance | Unofficial API, may break | Have Alpaca as backup |
| Polygon Basic | 5-year history limit | Supplement with Yahoo for older data |
| All sources | Missing data for delistings | Track delisting dates, handle gracefully |

#### Survivorship Bias

**Problem:** Current S&P 500 list excludes companies that were removed (often due to failure).

**Mitigation:**
```python
# Store historical index membership
CREATE TABLE index_membership (
    index_name VARCHAR,     -- 'SP500', 'SP400', etc.
    symbol VARCHAR,
    start_date DATE,        -- When added to index
    end_date DATE,          -- When removed (NULL if current)
    PRIMARY KEY (index_name, symbol, start_date)
);

# When backtesting, use point-in-time membership
def get_universe_at_date(index_name: str, as_of: date) -> list[str]:
    return query("""
        SELECT symbol FROM index_membership
        WHERE index_name = ?
          AND start_date <= ?
          AND (end_date IS NULL OR end_date > ?)
    """, index_name, as_of, as_of)
```

**Historical S&P 500 data sources:**
- Siblis Research (paid)
- Academic datasets
- Manual reconstruction from SEC filings (time-consuming)

For MVP: Accept this limitation, document it, focus on liquid large-caps that are unlikely to delist.

### Rate Limiting

```python
# hrp/data/rate_limiter.py
from functools import wraps
import time

class RateLimiter:
    def __init__(self, calls_per_minute: int):
        self.calls_per_minute = calls_per_minute
        self.min_interval = 60.0 / calls_per_minute
        self.last_call = 0

    def wait(self):
        elapsed = time.time() - self.last_call
        if elapsed < self.min_interval:
            time.sleep(self.min_interval - elapsed)
        self.last_call = time.time()

# Usage
polygon_limiter = RateLimiter(calls_per_minute=5)  # Basic tier

def fetch_with_rate_limit(symbol):
    polygon_limiter.wait()
    return polygon_source.get_daily_bars(symbol, ...)
```

### Initial Data Load

```bash
# scripts/initial_data_load.sh

# 1. Load S&P 500 universe
python -m hrp.data.ingestion.universe --index SP500

# 2. Backfill price history (will take time with rate limits)
python -m hrp.data.ingestion.prices --start 2010-01-01 --source polygon

# 3. Compute initial features
python -m hrp.data.ingestion.features --start 2010-01-01

# 4. Verify data quality
python -m hrp.data.quality.check --report
```

Estimated time for initial load (S&P 500, 15 years): 2-4 hours with rate limiting.

---

## Section 10: Directory Structure

### Repository Layout

```
hrp/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ CLAUDE.md                   # Claude Code project context
‚îú‚îÄ‚îÄ pyproject.toml              # Project metadata, dependencies
‚îú‚îÄ‚îÄ requirements.txt            # Pinned dependencies
‚îú‚îÄ‚îÄ requirements-dev.txt        # Dev dependencies
‚îú‚îÄ‚îÄ .env.example                # Template for environment variables
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ .pre-commit-config.yaml
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ plans/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 2025-01-19-hrp-spec.md   # This specification
‚îÇ   ‚îú‚îÄ‚îÄ hypotheses/             # Hypothesis documentation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ HYP-2025-001.md
‚îÇ   ‚îî‚îÄ‚îÄ decisions/              # Architecture Decision Records
‚îÇ       ‚îî‚îÄ‚îÄ ADR-001-duckdb.md
‚îÇ
‚îú‚îÄ‚îÄ hrp/                        # Main package
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ api/                    # Platform API layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ platform.py         # PlatformAPI class
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ data/                   # Data layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ db.py               # DuckDB connection management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schema.py           # Table definitions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sources/            # Data source adapters
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py         # Abstract base class
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ polygon_source.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ yfinance_source.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ alpaca_source.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ingestion/          # Data ingestion pipelines
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prices.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fundamentals.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ universe.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ features.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ quality/            # Data quality checks
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ checks.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rate_limiter.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ research/               # Research engine
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hypothesis.py       # Hypothesis registry
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ backtest.py         # VectorBT wrapper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.py          # Standard metrics calculation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ benchmark.py        # Benchmark comparison
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ lineage.py          # Audit trail
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ ml/                     # ML experiment framework
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py           # Model registry
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ features.py         # Feature engineering
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ training.py         # Training pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validation.py       # Walk-forward validation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ signals.py          # Prediction to signal conversion
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ risk/                   # Risk management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ limits.py           # Position/risk limits
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ drawdown.py         # Drawdown monitoring
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ costs.py            # Transaction cost model
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validation.py       # Statistical validation
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ dashboard/              # Streamlit dashboard
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app.py              # Main Streamlit app
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pages/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ home.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ data_health.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ universe.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ hypotheses.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ experiments.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ backtest.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ deployment.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ lineage.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ agents.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ settings.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ mcp/                    # MCP servers for Claude
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ research_server.py  # Research tools for Claude
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ agents/                 # Scheduled agents
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scheduler.py        # APScheduler setup
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_monitor.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ discovery_agent.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validation_agent.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ report_agent.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ notifications/          # Alerts and notifications
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ email.py
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/                  # Shared utilities
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ config.py           # Configuration management
‚îÇ       ‚îî‚îÄ‚îÄ logging.py          # Logging setup
‚îÇ
‚îú‚îÄ‚îÄ scripts/                    # CLI scripts
‚îÇ   ‚îú‚îÄ‚îÄ initial_data_load.sh
‚îÇ   ‚îú‚îÄ‚îÄ daily_update.sh
‚îÇ   ‚îú‚îÄ‚îÄ backup.sh
‚îÇ   ‚îî‚îÄ‚îÄ start_services.sh
‚îÇ
‚îú‚îÄ‚îÄ strategies/                 # Strategy implementations
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ base.py                 # Abstract strategy class
‚îÇ   ‚îú‚îÄ‚îÄ momentum/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ classic_momentum.py
‚îÇ   ‚îú‚îÄ‚îÄ mean_reversion/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ ml_based/
‚îÇ       ‚îî‚îÄ‚îÄ __init__.py
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                  # Jupyter notebooks (exploration)
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îÇ
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ conftest.py             # Pytest fixtures
    ‚îú‚îÄ‚îÄ test_api/
    ‚îú‚îÄ‚îÄ test_data/
    ‚îú‚îÄ‚îÄ test_research/
    ‚îú‚îÄ‚îÄ test_ml/
    ‚îú‚îÄ‚îÄ test_risk/
    ‚îî‚îÄ‚îÄ test_strategies/
```

### CLAUDE.md

Project context file for Claude Code. Located at repository root.

```markdown
# HRP - Hedgefund Research Platform

## Project Overview

Personal quantitative research platform for systematic trading strategy development.
Long-only US equities, daily timeframe, institutional rigor.

## Architecture

Three-layer architecture:
1. **Data Layer** - DuckDB storage, ingestion pipelines, feature store
2. **Research Layer** - VectorBT backtesting, MLflow experiments, hypothesis registry
3. **Control Layer** - Streamlit dashboard, MCP servers, scheduled agents

All external access goes through `hrp/api/platform.py`. Never access DuckDB directly.

## Key Principles

1. **Research-First** - Every strategy starts as a formal hypothesis
2. **Reproducibility** - All experiments versioned and re-runnable
3. **Statistical Rigor** - Walk-forward validation, significance testing enforced
4. **Audit Trail** - Full lineage from hypothesis to deployment

## Agent Permissions

| Action | Agent | User |
|--------|-------|------|
| Create/run hypotheses | ‚úÖ | ‚úÖ |
| Run backtests | ‚úÖ | ‚úÖ |
| Analyze results | ‚úÖ | ‚úÖ |
| **Deploy strategies** | ‚ùå | ‚úÖ |

Agents cannot approve deployments or modify deployed strategies.

## Code Conventions

- Python 3.11+
- Type hints required
- Black formatting (100 char line length)
- All database access through `hrp/api/platform.py`
- Log all significant actions to lineage table

## Common Tasks

### Run a backtest
```python
from hrp.api.platform import PlatformAPI
api = PlatformAPI()
experiment_id = api.run_backtest(config, hypothesis_id='HYP-2025-001')
```

### Create a hypothesis
```python
api.create_hypothesis(
    title="Momentum predicts returns",
    thesis="Stocks with high 12-month returns continue outperforming",
    prediction="Top decile momentum > SPY by 3% annually",
    falsification="Sharpe < SPY or p-value > 0.05",
    actor='user'  # or 'agent:discovery'
)
```

### Query data
```python
prices = api.get_prices(['AAPL', 'MSFT'], start_date, end_date)
features = api.get_features(['AAPL'], ['momentum_20d', 'volatility_60d'], date)
```

## File Locations

- Database: `~/hrp-data/hrp.duckdb`
- MLflow: `~/hrp-data/mlflow/`
- Logs: `~/hrp-data/logs/`

## Testing

```bash
pytest tests/ -v
```

## Services

| Service | Command | Port |
|---------|---------|------|
| Dashboard | `streamlit run hrp/dashboard/app.py` | 8501 |
| MLflow UI | `mlflow ui` | 5000 |

## Current Scope

- Universe: S&P 500 (excluding financials, REITs, penny stocks)
- Direction: Long-only
- Timeframe: Daily
- Broker: Interactive Brokers
```

### Data Directory (Outside Repo)

```
~/hrp-data/                     # Gitignored, not in repo
‚îú‚îÄ‚îÄ hrp.duckdb                  # Main database
‚îú‚îÄ‚îÄ mlflow/
‚îÇ   ‚îú‚îÄ‚îÄ mlruns/                 # MLflow experiment data
‚îÇ   ‚îî‚îÄ‚îÄ mlflow.db               # MLflow tracking database
‚îú‚îÄ‚îÄ backups/
‚îÇ   ‚îú‚îÄ‚îÄ hrp-20250119.duckdb
‚îÇ   ‚îî‚îÄ‚îÄ mlflow-20250119.tar.gz
‚îú‚îÄ‚îÄ cache/                      # API response cache
‚îÇ   ‚îî‚îÄ‚îÄ polygon/
‚îî‚îÄ‚îÄ logs/
    ‚îú‚îÄ‚îÄ ingestion.log
    ‚îî‚îÄ‚îÄ agents.log
```

### Key Files Explained

| File | Purpose |
|------|---------|
| `CLAUDE.md` | Project context for Claude Code |
| `hrp/api/platform.py` | Single entry point for all operations |
| `hrp/data/db.py` | DuckDB connection pool, query helpers |
| `hrp/research/hypothesis.py` | CRUD for hypothesis registry |
| `hrp/research/backtest.py` | VectorBT configuration and execution |
| `hrp/ml/training.py` | ML training pipeline with MLflow logging |
| `hrp/dashboard/app.py` | Streamlit multipage app entry point |
| `hrp/mcp/research_server.py` | MCP tools for Claude integration |
| `hrp/agents/scheduler.py` | Background job scheduling |

### Configuration Files

#### pyproject.toml

```toml
[project]
name = "hrp"
version = "0.1.0"
description = "Hedgefund Research Platform"
requires-python = ">=3.11"

[tool.black]
line-length = 100
target-version = ['py311']

[tool.ruff]
line-length = 100
select = ["E", "F", "I", "N", "W"]

[tool.mypy]
python_version = "3.11"
strict = true
```

#### .gitignore

```gitignore
# Python
__pycache__/
*.py[cod]
.venv/
*.egg-info/

# Data (stored outside repo)
*.duckdb
mlruns/

# Environment
.env

# IDE
.vscode/
.idea/

# OS
.DS_Store
```

#### .pre-commit-config.yaml

```yaml
repos:
  - repo: https://github.com/psf/black
    rev: 23.12.0
    hooks:
      - id: black

  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.8
    hooks:
      - id: ruff
        args: [--fix]

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.8.0
    hooks:
      - id: mypy
        additional_dependencies: [types-all]
```

### Module Dependencies

```
api/platform.py
    ‚îú‚îÄ‚îÄ data/db.py
    ‚îú‚îÄ‚îÄ research/hypothesis.py
    ‚îú‚îÄ‚îÄ research/backtest.py
    ‚îú‚îÄ‚îÄ research/lineage.py
    ‚îú‚îÄ‚îÄ ml/training.py
    ‚îî‚îÄ‚îÄ risk/limits.py

dashboard/app.py
    ‚îî‚îÄ‚îÄ api/platform.py

mcp/research_server.py
    ‚îî‚îÄ‚îÄ api/platform.py

agents/*.py
    ‚îî‚îÄ‚îÄ api/platform.py
```

All external consumers (dashboard, MCP, agents) go through `api/platform.py`. No direct database access.

---

## Section 11: Development Phases

### Overview

Phased approach prioritizing a working system over completeness. Each phase delivers usable functionality. Don't move to the next phase until the current one is stable.

---

### Phase 0: Foundation ‚úÖ COMPLETE

**Goal:** Project skeleton, development environment, basic data pipeline.

#### Deliverables

- [x] Repository setup (git, pyproject.toml, pre-commit)
- [x] CLAUDE.md and README.md
- [x] Virtual environment with core dependencies
- [x] DuckDB schema creation (`hrp/data/schema.py`)
- [x] Basic data source adapter (Yahoo Finance) (`hrp/data/sources/yfinance_source.py`)
- [x] Price ingestion for 10 test symbols
- [x] Verify data in DuckDB

#### Success Criteria

```bash
# Can run this and see data
python -c "
import duckdb
con = duckdb.connect('~/hrp-data/hrp.duckdb')
print(con.execute('SELECT COUNT(*) FROM prices').fetchone())
"
```

#### Exit Criteria

- [x] 10 symbols with 5+ years of price data loaded
- [x] Schema matches spec
- [x] All tests pass

---

### Phase 1: Core Research Loop ‚úÖ COMPLETE

**Goal:** Run a simple backtest end-to-end, log to MLflow.

#### Deliverables

- [x] Platform API skeleton (`hrp/api/platform.py` - 1200+ lines)
- [x] VectorBT backtest wrapper (`hrp/research/backtest.py`)
- [x] Standard metrics calculation (`hrp/research/metrics.py`)
- [x] MLflow integration (`hrp/research/mlflow_utils.py`)
- [x] Simple momentum strategy implementation
- [x] Benchmark comparison (SPY) (`hrp/research/benchmark.py`)

#### Success Criteria

```bash
# Can run a backtest and see results in MLflow
python -m hrp.research.backtest --strategy momentum --start 2015-01-01
mlflow ui  # View results at localhost:5000
```

#### Exit Criteria

- [x] Backtest runs without errors
- [x] Results logged to MLflow with params, metrics, artifacts
- [x] Equity curve artifact generated
- [x] Benchmark comparison included

---

### Phase 2: Hypothesis & Lineage ‚úÖ COMPLETE

**Goal:** Formal hypothesis workflow with audit trail.

#### Deliverables

- [x] Hypothesis registry (`hrp/research/hypothesis.py`)
- [x] Lineage system (`hrp/research/lineage.py`)
- [x] Link hypotheses to experiments
- [x] Hypothesis CRUD via Platform API
- [x] Basic validation criteria checks

#### Success Criteria

```python
# Can create hypothesis and link to experiment
api = PlatformAPI()
hyp_id = api.create_hypothesis(title="...", thesis="...", ...)
exp_id = api.run_backtest(config, hypothesis_id=hyp_id)
lineage = api.get_lineage(hypothesis_id=hyp_id)
assert len(lineage) >= 2  # creation + experiment
```

#### Exit Criteria

- [x] Hypothesis lifecycle works (draft ‚Üí testing ‚Üí validated/rejected)
- [x] All experiments linked to hypotheses
- [x] Lineage queries return correct chains

---

### Phase 3: Dashboard MVP ‚úÖ COMPLETE

**Goal:** Basic Streamlit dashboard for visibility.

#### Deliverables

- [x] Streamlit app skeleton (`hrp/dashboard/app.py`)
- [x] Home page (system status) (`hrp/dashboard/pages/home.py`)
- [x] Data Health page (ingestion status) (`hrp/dashboard/pages/data_health.py`, `ingestion_status.py`)
- [x] Hypotheses page (list, create, view) (`hrp/dashboard/pages/hypotheses.py`)
- [x] Experiments page (MLflow integration) (`hrp/dashboard/pages/experiments.py`)

#### Success Criteria

```bash
streamlit run hrp/dashboard/app.py
# Navigate to localhost:8501, see data, create hypothesis
```

#### Exit Criteria

- [x] Dashboard loads without errors
- [x] Can view hypotheses and experiments
- [x] Can trigger backtest from UI

---

### Phase 4: Full Data Pipeline ‚úÖ COMPLETE

**Goal:** Production-ready data ingestion with quality checks.

**Status:** ‚úÖ All core deliverables complete. Two optional enhancements remain (OpenBB, incremental compute).

#### Deliverables

- [x] S&P 500 universe management (`hrp/data/universe.py`) ‚úÖ
- [x] Polygon.io integration (`hrp/data/sources/polygon_source.py`) ‚úÖ
- [x] Feature store implementation (`hrp/data/ingestion/features.py` - 14+ features) ‚úÖ
- [x] Data quality checks (`hrp/data/quality/checks.py` - 5 checks) ‚úÖ
- [x] Scheduled ingestion with APScheduler (`hrp/agents/scheduler.py`, `hrp/agents/jobs.py`) ‚úÖ
- [x] Universe filtering (exclude financials, REITs, penny stocks) ‚úÖ

#### Bonus Implementations (All Complete)
- [x] YFinance fallback source (`hrp/data/sources/yfinance_source.py`) ‚úÖ
- [x] Corporate actions support (splits, dividends) ‚úÖ
- [x] Quality report storage and health trends (`hrp/data/quality/report.py`) ‚úÖ
- [x] Email notifications on job failure (`hrp/notifications/email.py`) ‚úÖ
- [x] CLI interface (`hrp/agents/cli.py`) ‚úÖ
- [x] Retry logic with exponential backoff (`hrp/utils/retry.py`) ‚úÖ
- [x] Automated backup system (`hrp/data/backup.py`) ‚úÖ
- [x] Historical data backfill (`hrp/data/backfill.py`) ‚úÖ
- [x] Feature versioning (`hrp/data/features/registry.py`) ‚úÖ

#### Success Criteria

```bash
# Scheduled job runs and updates data
python -m hrp.agents.cli run-now --job prices
python -m hrp.agents.cli run-now --job features
python -m hrp.agents.cli list-jobs

# Backup operations
python -m hrp.data.backup --backup
python -m hrp.data.backup --list

# Historical backfill
python -m hrp.data.backfill --symbols AAPL MSFT --start 2020-01-01 --all
```

#### Exit Criteria

- [x] Full S&P 500 universe loaded ‚úÖ
- [x] Features computed and stored ‚úÖ
- [x] Quality checks passing ‚úÖ
- [x] Daily updates automated ‚úÖ
- [x] Backup system operational ‚úÖ
- [x] Backfill capability working ‚úÖ

#### Optional Enhancements (Not Blockers)

- [ ] OpenBB SDK integration ‚Äî Would unify data sources
- [ ] Incremental feature computation ‚Äî Performance optimization for large-scale updates

---

### Phase 5: ML Framework ‚úÖ COMPLETE

**Goal:** ML training pipeline with proper validation.

**Status:** All MVP and enhanced deliverables complete as of January 2026.

#### Deliverables (MVP)

- [x] ML model registry (`hrp/ml/models.py`) - Ridge, Lasso, ElasticNet, RandomForest, MLP + optional LightGBM/XGBoost
- [x] Training pipeline (`hrp/ml/training.py`) - Data loading, train/val/test splits, metrics
- [x] Walk-forward validation (`hrp/ml/validation.py`) - Expanding/rolling windows, stability score
- [x] Feature selection (`hrp/ml/training.py:select_features`) - Mutual information based
- [x] Signal generation from predictions (`hrp/ml/signals.py`) - rank, threshold, zscore methods
- [x] Overfitting guards (test set discipline) - Complete (`hrp/risk/overfitting.py`)

#### Success Criteria

```python
# Can train model and generate signals
from hrp.ml import MLConfig, train_model, predictions_to_signals

config = MLConfig(
    model_type='ridge',
    target='returns_20d',
    features=['momentum_20d', 'volatility_20d'],
    train_start=date(2015, 1, 1),
    train_end=date(2018, 12, 31),
    validation_start=date(2019, 1, 1),
    validation_end=date(2019, 12, 31),
    test_start=date(2020, 1, 1),
    test_end=date(2023, 12, 31),
)
result = train_model(config, symbols=['AAPL', 'MSFT'])
signals = predictions_to_signals(predictions_df, method='rank', top_pct=0.1)
```

#### Exit Criteria (MVP)

- [x] Linear models (Ridge, Lasso) working
- [x] Tree-based models (RandomForest) working
- [x] Walk-forward validation implemented
- [x] Test set guard enforced (see docs/plans/2025-01-22-overfitting-guards-COMPLETE.md)
- [x] ML experiments logged to MLflow (implemented in training.py and validation.py)

---

### Phase 6: Agent Integration

**Goal:** Claude can run research via MCP.

#### Deliverables

- [ ] MCP server (`hrp/mcp/research_server.py`)
- [ ] Tools: list_hypotheses, create_hypothesis, run_backtest, analyze_results
- [ ] Claude Code configuration
- [ ] Actor tracking (agent vs user)
- [ ] Deployment permission enforcement

#### Success Criteria

```
# In Claude Code session with MCP connected
> List all hypotheses
> Create a new hypothesis about RSI mean reversion
> Run a backtest for it
> Analyze the results
```

#### Exit Criteria

- Claude can perform full research loop
- All actions logged with correct actor
- Deployment blocked for agents

---

### Phase 7: Scheduled Agents

**Goal:** Autonomous background research.

**Status:** ‚úÖ Scheduler infrastructure complete (Jan 2026), research agents pending

#### Deliverables

- [x] Scheduler setup (`hrp/agents/scheduler.py`) ‚úÖ
- [x] Job infrastructure (`hrp/agents/jobs.py` - PriceIngestionJob, UniverseUpdateJob, FeatureComputationJob) ‚úÖ
- [x] CLI interface (`hrp/agents/cli.py`) ‚úÖ
- [x] Email notifications (`hrp/notifications/email.py`) ‚úÖ
- [x] **Production Deployment** (Jan 24, 2026):
  - launchd background service (PID 94352)
  - Daily pipeline: Prices (18:00) ‚Üí Universe (18:05) ‚Üí Features (18:10) ‚Üí Backup (02:00)
  - Comprehensive monitoring infrastructure
- [ ] Data Monitor agent (future)
- [ ] Discovery agent (hypothesis generation)
- [ ] Validation agent (robustness checks)
- [ ] Report agent (weekly summary)

#### Success Criteria

```bash
# Scheduler infrastructure working
launchctl list | grep hrp
python -m hrp.agents.cli list-jobs
python -m hrp.agents.cli run-now --job universe

# Health monitoring
python ~/hrp-data/scripts/check_universe_health.py
```

**Note:** Job scheduling infrastructure is complete and deployed. Research agents (discovery, validation, report) are planned for future implementation.

#### Exit Criteria (Scheduler Infrastructure)

- [x] Jobs run on schedule without errors ‚úÖ
- [x] Email notifications working ‚úÖ
- [x] Production service deployed and monitored ‚úÖ
- [ ] Discovery agent creates valid hypotheses (future)

---

### Phase 8: Risk & Validation

**Goal:** Full statistical validation framework.

**Status:** üü° 70% complete (Jan 2026)

#### Deliverables

- [x] Walk-forward validation (`hrp/ml/validation.py`) ‚úÖ
- [x] Statistical significance testing (t-tests, bootstrap CI) ‚úÖ
- [x] Multiple hypothesis correction (Bonferroni, Benjamini-Hochberg) ‚úÖ
- [x] Robustness checks (parameter sensitivity, regime analysis) (`hrp/risk/robustness.py`) ‚úÖ
- [x] Test set discipline (TestSetGuard with 3-evaluation limit) (`hrp/risk/overfitting.py`) ‚úÖ
- [x] Strategy validation gates (`hrp/risk/validation.py`) ‚úÖ
- [ ] Position limits enforcement (future)
- [ ] Drawdown monitoring (future)
- [ ] Transaction cost model (IBKR) (future)
- [ ] Enhanced validation reports (future)

#### Success Criteria

```python
# Walk-forward validation
from hrp.ml import walk_forward_validate, WalkForwardConfig
result = walk_forward_validate(config, symbols=['AAPL', 'MSFT'])
print(f"Stable: {result.is_stable}")  # stability_score <= 1.0

# Test set discipline
from hrp.risk import TestSetGuard
guard = TestSetGuard(hypothesis_id='HYP-2025-001')
with guard.evaluate(metadata={"experiment": "final"}):
    metrics = model.evaluate(test_data)

# Strategy validation
from hrp.risk.validation import validate_strategy
result = validate_strategy({"sharpe": 0.80, "num_trades": 200})
print(f"Passed: {result.passed}")
```

#### Exit Criteria

- [x] Cannot exceed 3 test set evaluations per hypothesis ‚úÖ
- [x] Validation gates enforce minimum criteria ‚úÖ
- [x] Robustness report generated automatically ‚úÖ
- [ ] Risk limits enforced in backtests (future)

---

### Phase 9: Paper Trading (Future)

**Goal:** Deploy validated strategies to paper trading.

#### Deliverables

- [ ] IBKR paper trading connection
- [ ] Position tracking
- [ ] Order execution
- [ ] Live vs backtest comparison
- [ ] Performance monitoring

**Note:** This phase is out of initial scope. Build only after Phase 8 is stable and you have validated strategies worth testing.

---

### Phase Summary

| Phase | Focus | Key Outcome | Status (Jan 2026) |
|-------|-------|-------------|-------------------|
| 0 | Foundation | Data loads into DuckDB | ‚úÖ Complete |
| 1 | Core Research | Backtest runs, logs to MLflow | ‚úÖ Complete |
| 2 | Hypothesis | Formal research workflow | ‚úÖ Complete |
| 3 | Dashboard | Visual interface | ‚úÖ Complete |
| 4 | Data Pipeline | Production data quality | ‚úÖ Complete (Universe deployed Jan 24) |
| 5 | ML Framework | ML training with validation | ‚úÖ Complete |
| 6 | Agent Integration | Claude runs research | üü° 60% (MCP pending) |
| 7 | Scheduled Agents | Autonomous discovery | üü° Infrastructure complete, agents pending |
| 8 | Risk & Validation | Statistical rigor enforced | üü° 70% (Core validation complete) |
| 9 | Paper Trading | Live deployment (future) | ‚è≠Ô∏è Future phase |

### Recommended Approach

1. **Complete Phase 0-3 first** ‚Äî This gives you a working research loop with visibility
2. **Use the system** ‚Äî Run real research, find pain points
3. **Then prioritize** ‚Äî Maybe ML (Phase 5) matters more than scheduled agents (Phase 7) for you
4. **Iterate** ‚Äî Each phase will reveal requirements you didn't anticipate

### Anti-Patterns to Avoid

| Anti-Pattern | Why It's Bad | Instead |
|--------------|--------------|---------|
| Building all phases before using | Never discover real needs | Ship Phase 3, then iterate |
| Perfecting data pipeline before any backtests | Paralysis | Use Yahoo Finance, upgrade later |
| Building ML before simple strategies work | Complexity too early | Momentum baseline first |
| Optimizing performance before it's slow | Premature optimization | Profile first, optimize second |
| Adding features before core works | Scope creep | Finish phases in order |
